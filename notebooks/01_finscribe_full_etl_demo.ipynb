{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FinScribe ETL Pipeline - Full Demo\n",
        "\n",
        "This notebook demonstrates the complete ETL pipeline for financial document processing:\n",
        "\n",
        "1. **Extract**: Ingest documents from various sources\n",
        "2. **Transform**: OCR, semantic parsing, normalization\n",
        "3. **Load**: Store structured data in PostgreSQL + MinIO\n",
        "4. **Validate**: Business rule validation\n",
        "\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q opencv-python pillow psycopg2-binary dateparser requests boto3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up working directory\n",
        "import os\n",
        "\n",
        "# Try to detect if we're in the notebooks/ subdirectory\n",
        "current_dir = os.getcwd()\n",
        "if os.path.basename(current_dir) == 'notebooks':\n",
        "    # We're in notebooks/, go up to project root\n",
        "    os.chdir('..')\n",
        "elif os.path.basename(current_dir) != 'finscribe-smart-scan':\n",
        "    # Check if we're already in project root by looking for data_pipeline/\n",
        "    if not os.path.exists('data_pipeline'):\n",
        "        # Try going up one level\n",
        "        if os.path.exists('..') and os.path.exists(os.path.join('..', 'data_pipeline')):\n",
        "            os.chdir('..')\n",
        "\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "print(f\"data_pipeline exists: {os.path.exists('data_pipeline')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import ETL Pipeline Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add project root to Python path\n",
        "project_root = os.getcwd()\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "from data_pipeline.ingestion import ingest_from_local\n",
        "from data_pipeline.preprocess import preprocess\n",
        "from data_pipeline.ocr_client import run_ocr\n",
        "from data_pipeline.semantic_parser import parse\n",
        "from data_pipeline.normalizer import normalize_invoice_data\n",
        "from data_pipeline.validator import validate\n",
        "\n",
        "print(\"ETL modules imported successfully\")\n",
        "print(f\"Project root: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run ETL Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process a single invoice\n",
        "import os\n",
        "\n",
        "# Use absolute path to ensure we find the file\n",
        "project_root = os.getcwd()\n",
        "invoice_path = os.path.join(project_root, \"examples\", \"sample_invoice_1.png\")\n",
        "\n",
        "if not os.path.exists(invoice_path):\n",
        "    print(f\"Invoice not found: {invoice_path}\")\n",
        "    print(\"Please add a sample invoice to the examples/ directory\")\n",
        "    print(f\"Current working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"Processing: {invoice_path}\")\n",
        "    \n",
        "    try:\n",
        "        # Extract\n",
        "        print(\"\\n[1/5] Ingesting...\")\n",
        "        src = ingest_from_local(invoice_path)\n",
        "        \n",
        "        print(\"\\n[2/5] Preprocessing...\")\n",
        "        clean = preprocess(src)\n",
        "        \n",
        "        print(\"\\n[3/5] Running OCR...\")\n",
        "        ocr_result = run_ocr(clean)\n",
        "        \n",
        "        print(\"\\n[4/5] Parsing semantic structure...\")\n",
        "        parsed = parse(ocr_result)\n",
        "        normalized = normalize_invoice_data(parsed)\n",
        "        \n",
        "        print(\"\\n[5/5] Validating...\")\n",
        "        validation = validate(normalized)\n",
        "        \n",
        "        print(\"\\n✅ ETL Pipeline Complete!\")\n",
        "        print(f\"Validation: {'PASSED' if validation['ok'] else 'FAILED'}\")\n",
        "        if not validation['ok']:\n",
        "            print(f\"Errors: {validation['errors']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error during ETL pipeline: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RAW OCR TEXT (first 500 chars):\")\n",
        "print(\"=\" * 60)\n",
        "if 'ocr_result' in locals():\n",
        "    print(ocr_result.get(\"text\", \"\")[:500])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STRUCTURED JSON:\")\n",
        "print(\"=\" * 60)\n",
        "if 'normalized' in locals():\n",
        "    print(json.dumps(normalized, indent=2))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
